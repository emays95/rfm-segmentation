{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "350f7ed3-4797-4844-a48e-03f58863af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, pandas as ps\n",
    "from pyspark.sql import SparkSession, functions as sf\n",
    "from pyspark.sql.functions import col, collect_list\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77bf7037-b9fb-456d-98f0-956780e81bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.driver.memory\", \"8g\")\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .appName(\"Spark UI Tutorial\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42e1d65c-6233-4ccc-b1b0-f2ebd3c1db2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Transaction_ID=1000000000, Date=datetime.datetime(2022, 1, 21, 6, 27, 29), Customer_Name='Stacey Price', Product=\"['Ketchup', 'Shaving Cream', 'Light Bulbs']\", Total_Items=3, Total_Cost=71.65, Payment_Method='Mobile Payment', City='Los Angeles', Store_Type='Warehouse Club', Discount_Applied=True, Customer_Category='Homemaker', Season='Winter', Promotion='None')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# schema = StructType([\n",
    "#     StructField(\"Transaction_ID\", IntegerType(), True),\n",
    "#     StructField(\"Date\", DateType(), True),\n",
    "#     StructField(\"Customer_Name\", StringType(), True),\n",
    "#     StructField(\"Product\", ArrayType(StringType()), True),\n",
    "#     StructField(\"Total_Items\", IntegerType(), True),\n",
    "#     StructField(\"Total_Cost\", DecimalType(), True),\n",
    "#     StructField(\"Payment_Method\", StringType(), True),\n",
    "#     StructField(\"City\", StringType(), True),\n",
    "#     StructField(\"Store_Type\", StringType(), True),\n",
    "#     StructField(\"Discount_Applied\", BooleanType(), True),\n",
    "#     StructField(\"Customer_Category\", StringType(), True),\n",
    "#     StructField(\"Season\", StringType(), True),\n",
    "#     StructField(\"Promotion\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "\n",
    "df = spark_session.read.csv('Retail_Transactions_Dataset.csv', header=True, inferSchema=True, quote='\"')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "069aa035-f25e-4915-acb0-39124f5e4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.Product = df.Product.cast(ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edecd490-bf3b-4c16-b799-3ce5284d4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = df[['Customer_Name', 'Customer_Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5721b40c-0001-4c41-9fe5-37bdd40bf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Customer_Name='Stacey Price', Customer_Category='Homemaker'),\n",
       " Row(Customer_Name='Michelle Carlson', Customer_Category='Professional'),\n",
       " Row(Customer_Name='Lisa Graves', Customer_Category='Professional'),\n",
       " Row(Customer_Name='Mrs. Patricia May', Customer_Category='Homemaker'),\n",
       " Row(Customer_Name='Susan Mitchell', Customer_Category='Young Adult'),\n",
       " Row(Customer_Name='Joshua Frazier', Customer_Category='Retiree'),\n",
       " Row(Customer_Name='Victoria Garrett', Customer_Category='Student'),\n",
       " Row(Customer_Name='Sydney Waller', Customer_Category='Young Adult'),\n",
       " Row(Customer_Name='Kimberly Morgan', Customer_Category='Homemaker'),\n",
       " Row(Customer_Name='Lori Conway', Customer_Category='Young Adult')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21ff7408-1ffb-4b3a-9bbb-1b59f404cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Transaction_ID=1000000000, Product=\"['Ketchup'\"),\n",
       " Row(Transaction_ID=1000000000, Product=\" 'Shaving Cream'\"),\n",
       " Row(Transaction_ID=1000000000, Product=\" 'Light Bulbs']\"),\n",
       " Row(Transaction_ID=1000000001, Product=\"['Ice Cream'\"),\n",
       " Row(Transaction_ID=1000000001, Product=\" 'Milk'\"),\n",
       " Row(Transaction_ID=1000000001, Product=\" 'Olive Oil'\"),\n",
       " Row(Transaction_ID=1000000001, Product=\" 'Bread'\"),\n",
       " Row(Transaction_ID=1000000001, Product=\" 'Potatoes']\"),\n",
       " Row(Transaction_ID=1000000002, Product=\"['Spinach']\"),\n",
       " Row(Transaction_ID=1000000003, Product=\"['Tissues'\")]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # = sf.split(df.Product,',')\n",
    "# print(products)\n",
    "# prods = [sf.split(p, ',').alias(\"Product\") for p in products]\n",
    "# print(prods)\n",
    "tran_prod = df.select(\"Transaction_ID\", sf.explode(sf.split(df.Product,',')).alias(\"Product\"))\n",
    "tran_prod.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5352aea-2e54-4b6c-a0de-9f26c915eebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'explode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtran_prod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct\u001b[39m\u001b[38;5;124m'\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m \n\u001b[1;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3125\u001b[0m     )\n\u001b[1;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'explode'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9ff1c56-9e2a-4092-9c15-083ea39b209b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/frame.py:13566\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m  13565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m> 13566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m  13567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/indexing.py:487\u001b[0m, in \u001b[0;36mLocIndexerLike.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    480\u001b[0m cond, limit, remaining_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_rows(rows_sel)\n\u001b[1;32m    481\u001b[0m (\n\u001b[1;32m    482\u001b[0m     column_labels,\n\u001b[1;32m    483\u001b[0m     data_spark_columns,\n\u001b[1;32m    484\u001b[0m     data_fields,\n\u001b[1;32m    485\u001b[0m     returns_series,\n\u001b[1;32m    486\u001b[0m     series_name,\n\u001b[0;32m--> 487\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols_sel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cond \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m returns_series:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/indexing.py:334\u001b[0m, in \u001b[0;36mLocIndexerLike._select_cols\u001b[0;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_cols_else\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/indexing.py:1392\u001b[0m, in \u001b[0;36mLocIndexer._select_cols_else\u001b[0;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     cols_sel \u001b[38;5;241m=\u001b[39m (cols_sel,)\n\u001b[0;32m-> 1392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_from_multiindex_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/indexing.py:1210\u001b[0m, in \u001b[0;36mLocIndexer._get_from_multiindex_column\u001b[0;34m(self, key, missing_keys, labels, recursed)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(k)\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'select'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1938/1900190526.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtran_prod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtran_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Product'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/pandas/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m  13564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13566\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13567\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 13568\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m  13569\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13570\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "tran_prod.select('*', sf.explode(tran_prod['Product'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdec77e-7eb6-4dc0-a671-0e618cf04e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
